rm(list = ls())
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"xgboost",
"caret",
"data.table"
)
# Cargar el conjunto de datos mtcars
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Definir los parámetros iniciales para el modelo XGBoost
xgb_params <- list(
objective = "multi:softmax",  # Clasificación multiclase
num_class = length(unique(y)),  # Número de clases
eval_metric = "mlogloss"  # Métrica para la pérdida de clasificación
)
# Crear una lista de parámetros para afinar (subsample)
grid <- expand.grid(
nrounds = 100,                   # Número de iteraciones (árboles)
max_depth = 6,                   # Profundidad máxima de los árboles
eta = 0.1,                       # Tasa de aprendizaje (shrinkage)
gamma = 0,                       # Reducción de la ganancia mínima para hacer una división
colsample_bytree = 0.8,          # Fracción de columnas usadas por árbol
subsample = c(0.5, 0.7, 0.9)     # Valores de muestreo que queremos afinar
)
# Configurar la validación cruzada estratificada
train_control <- trainControl(
method = "cv",                   # Validación cruzada
number = 5,                      # Número de particiones (folds)
verboseIter = TRUE,              # Mostrar progreso
allowParallel = TRUE             # Permitir paralelización
)
# Realizar la búsqueda de los mejores parámetros con GridSearchCV
xgb_tune <- train(
X, y,                            # Datos de entrenamiento
method = "xgbTree",              # Método XGBoost
trControl = train_control,       # Control para validación cruzada
tuneGrid = grid,                 # Conjunto de parámetros a afinar
metric = "Accuracy",             # Métrica a optimizar
verbose = FALSE
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- fread("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula
data = data,                       # Conjunto de datos
distribution = "gaussian",         # Distribución (cambiar según tu problema)
n.trees = 10000,                   # Número máximo de árboles (ajustar según validación)
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- fread("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
View(data)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula
data = data,                       # Conjunto de datos
distribution = "gaussian",         # Distribución (cambiar según tu problema)
n.trees = 10000,                   # Número máximo de árboles (ajustar según validación)
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
# Hacer predicciones con el número óptimo de árboles
predicciones <- predict(modelo_gbm, newdata = data, n.trees = mejor_n_trees)
# Evaluar el modelo (cambiar según tu métrica)
mse <- mean((data$y - predicciones)^2)  # Error cuadrático medio
print(mse)  # Imprimir el error
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Agregar 'y' de nuevo al conjunto de datos 'data'
data$y <- y
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula
data = data,                       # Conjunto de datos
distribution = "gaussian",         # Distribución (cambiar según tu problema)
n.trees = 10000,                   # Número máximo de árboles (ajustar según validación)
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Agregar 'y' de nuevo al conjunto de datos 'data'
data$y <- y
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y' que ahora está en 'data'
data = data,                       # Conjunto de datos
distribution = "bernoulli",         # Usar 'bernoulli' para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
# Hacer predicciones con el número óptimo de árboles
predicciones <- predict(modelo_gbm, newdata = data, n.trees = mejor_n_trees)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV (fread es más rápido para archivos grandes)
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Agregar 'y' de nuevo al conjunto de datos 'data'
data$y <- y
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y' que ahora está en 'data'
data = data,                       # Conjunto de datos
distribution = "multinomial",         # Usar 'bernoulli' para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
# Agregar 'y' de nuevo al conjunto de datos 'data'
data$y <- y
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",      # Para clasificación multiclase
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
str(data[, 95])
# Codificar las clases de la variable objetivo
y <- as.numeric(as.factor(y)) - 1  # XGBoost requiere que la variable objetivo sea numérica
str(data[, 95])
# Agregar 'y' de nuevo al conjunto de datos 'data'
data$y <- as.factor(data[, 95])
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",      # Para clasificación multiclase
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",      # Para clasificación multiclase
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
str(data[, 95])
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
# y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
data$y <- as.factor(data[, 95])
str(data$y)
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "bernoulli",        # Para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
View(data)
View(data)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
# Dividir los datos en variables predictoras (X) y objetivo (y)
X <- as.matrix(data[, 1:94])  # Las primeras 94 columnas son características
# y <- as.factor(data[, 95])    # La columna 95 es la variable objetivo
data$y <- as.factor(data[, 95])
str(data$y)
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",        # Para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table",
"doParallel",
"readr",
"ggplot2"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
dataset <- as.matrix(data)
# Dividir los datos en X y y
X <- dataset[, 1:94]
y <- dataset[, 95]
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",        # Para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
y <- as.factor(dataset[, 95])
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos
distribution = "multinomial",        # Para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
rm(list = ls())
# Cargar las librerías necesarias usando pacman para verificar instalación
pacman::p_load(
"tidyverse",
"readxl",
"openxlsx",
"gbm",
"caret",
"data.table",
"doParallel",
"readr",
"ggplot2"
)
# Cargar el archivo CSV
data <- read.csv("E:/TESISXGBoost/Tesis/NotasXGBoost/DataFrames/train.csv")
str(df) # Tipos de datos
str(data) # Tipos de datos
data <- mutate_if(data, is.character, as.factor)
str(data) # Tipos de datos
# Dividir los datos en X y y
X <- data[, 1:94]
y <- data[, 95]
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
# Entrenar el modelo usando la validación cruzada
# Entrenar el modelo usando la validación cruzada
set.seed(123)  # Fijar semilla para reproducibilidad
modelo_gbm <- gbm(
formula = y ~ .,                   # Especificar la fórmula con 'y'
data = data,                       # Conjunto de datos        # Para clasificación binaria
n.trees = 10000,                   # Número máximo de árboles
interaction.depth = 3,             # Profundidad de los árboles
shrinkage = 0.001,                 # Tasa de aprendizaje
n.minobsinnode = 10,               # Mínimo de observaciones en nodos terminales
cv.folds = 5,                      # Número de folds para validación cruzada
n.cores = NULL,                    # Usar todos los núcleos disponibles
verbose = FALSE                    # Sin mostrar detalles durante el ajuste
)
